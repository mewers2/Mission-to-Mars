{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "440ffbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scraping tools\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d58b544",
   "metadata": {},
   "source": [
    "With these two lines of code, we are creating an instance of a Splinter browser. This means that we're prepping our automated browser. We're also specifying that we'll be using Chrome as our browser. **executable_path is unpacking the dictionary we've stored the path in – think of it as unpacking a suitcase. headless=False means that all of the browser's actions will be displayed in a Chrome window so we can see them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00f16639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Downloading: 100%|█████████████████| 6.21M/6.21M [00:17<00:00, 380kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Set up Splinter\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "369f983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit the Quotes to Scrape site\n",
    "url = 'http://books.toscrape.com/'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b59cb08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BeautifulSoup to parse the HTML: that means that BeautifulSoup has taken a look at the\n",
    "# different components and can now access them.  Specifically, BS parses the HTML text and then\n",
    "# stores it as an object.\n",
    "html = browser.html\n",
    "html_soup = soup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "721e3724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Light in the ...'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrape the Title\n",
    "title = html_soup.find('h3').text\n",
    "title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19485645",
   "metadata": {},
   "source": [
    "(reference the cell above)\n",
    "What we've just done in the last two lines of code is:\n",
    "    1. We used our html_soup object we created earlier and chained find() to it to search for the <h2 /> tag.\n",
    "    2. We've also extracted only the text within the HTML tags by adding .text to the end of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19b9830c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A Light in the ...\n",
      "\n",
      "Tipping the Velvet\n",
      "\n",
      "Soumission\n",
      "\n",
      "Sharp Objects\n",
      "\n",
      "Sapiens: A Brief History ...\n",
      "\n",
      "The Requiem Red\n",
      "\n",
      "The Dirty Little Secrets ...\n",
      "\n",
      "The Coming Woman: A ...\n",
      "\n",
      "The Boys in the ...\n",
      "\n",
      "The Black Maria\n",
      "\n",
      "Starving Hearts (Triangular Trade ...\n",
      "\n",
      "Shakespeare's Sonnets\n",
      "\n",
      "Set Me Free\n",
      "\n",
      "Scott Pilgrim's Precious Little ...\n",
      "\n",
      "Rip it Up and ...\n",
      "\n",
      "Our Band Could Be ...\n",
      "\n",
      "Olio\n",
      "\n",
      "Mesaerion: The Best Science ...\n",
      "\n",
      "Libertarianism for Beginners\n",
      "\n",
      "It's Only the Himalayas\n"
     ]
    }
   ],
   "source": [
    "# Scrape the top ten tags\n",
    "tag_box = html_soup.find('ol', class_=\"row\")\n",
    "# tag_box\n",
    "tags = tag_box.find_all('a')\n",
    "\n",
    "for tag in tags:\n",
    "    word = tag.text\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d049f762",
   "metadata": {},
   "source": [
    "(reference the cell above)\n",
    "This code looks really similar to our last, but we've increased the difficulty a bit by incorporating a for loop, but let's start at the beginning.\n",
    "\n",
    "The first line, tag_box = html_soup.find('div', class_='tags-box'), creates a new variable tag_box, which will be used to store the results of a search. In this case, we're looking for <div /> elements with a class of tags-box, and we're searching for it in the HTML we parsed earlier and stored in the html_soup variable.\n",
    "\n",
    "The second line, tags = tag_box.find_all('a', class_='tag'), is similar to the first but with a few tweaks to make the search more specific. The new \"tags\" variable will hold the results of a find_all, but this time we're searching through the parsed results stored in our tag_box variable to find <a /> elements with a tag class.\n",
    "\n",
    "We used find_all this time because we want to capture all results, instead of a single or specific one.\n",
    "\n",
    "Next, we've added a for loop. This for loop cycles through each tag in the tags variable, strips the HTML code out of it, and then prints only the text of each tag."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
